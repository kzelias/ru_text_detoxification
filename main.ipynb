{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0"
      },
      "source": [
        "## Russian Text Detoxification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gocizGu_aiyJ",
        "outputId": "853f0812-eca4-4d21-9e26-17803a191084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHww1bJRUHoi"
      },
      "outputs": [],
      "source": [
        "# VRAM <= 8Gb\n",
        "# os.environ['PYTORCH_CUDA_ALLOC_CONF']  = 'max_split_size_mb:4096'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Eduv2TeUHoj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration, AutoTokenizer, AutoModel\n",
        "from transformers.file_utils import cached_property\n",
        "from typing import Tuple, List, Dict, Union\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "from tqdm.auto import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tISq3abmUHoj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/data/input/train.tsv', index_col=0, sep='\\t')\n",
        "df = df.fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BkjM0IdUHok"
      },
      "outputs": [],
      "source": [
        "df_train_toxic = []\n",
        "df_train_neutral = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    references = row[['neutral_comment1', 'neutral_comment2', 'neutral_comment3']].tolist()\n",
        "    \n",
        "    for reference in references:\n",
        "        if len(reference) > 0:\n",
        "            df_train_toxic.append(row['toxic_comment'])\n",
        "            df_train_neutral.append(reference)\n",
        "        else:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGFHJWpbUHok"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\n",
        "    'toxic_comment': df_train_toxic,\n",
        "    'neutral_comment': df_train_neutral\n",
        "})\n",
        "\n",
        "df = shuffle(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwI2Wdw9UHol"
      },
      "source": [
        "### TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmRuxNTEUHom"
      },
      "outputs": [],
      "source": [
        "class PairsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx < len(self.x['input_ids'])\n",
        "        item = {key: val[idx] for key, val in self.x.items()}\n",
        "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
        "        item['labels'] = self.y['input_ids'][idx]\n",
        "        return item\n",
        "    \n",
        "    @property\n",
        "    def n(self):\n",
        "        return len(self.x['input_ids'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n # * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STOdS123UHom"
      },
      "outputs": [],
      "source": [
        "class DataCollatorWithPadding:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        batch = self.tokenizer.pad(\n",
        "            features,\n",
        "            padding=True,\n",
        "        )\n",
        "        ybatch = self.tokenizer.pad(\n",
        "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
        "            padding=True,\n",
        "        ) \n",
        "        batch['labels'] = ybatch['input_ids']\n",
        "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
        "        \n",
        "        return {k: torch.tensor(v) for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRPBVAhlUHom"
      },
      "outputs": [],
      "source": [
        "def cleanup():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "cleanup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KHAqKoCUHon"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_dataloader):\n",
        "    num = 0\n",
        "    den = 0\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        with torch.no_grad():\n",
        "            loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
        "            num += len(batch) * loss.item()\n",
        "            den += len(batch)\n",
        "    val_loss = num / den\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWSxFVfxUHor"
      },
      "outputs": [],
      "source": [
        "def train_loop(\n",
        "    model, train_dataloader, val_dataloader, \n",
        "    max_epochs=10, \n",
        "    max_steps=5_00, \n",
        "    lr=2e-5,\n",
        "    gradient_accumulation_steps=1, \n",
        "    cleanup_step=100,\n",
        "    report_step=300,\n",
        "    window=100,\n",
        "):\n",
        "    cleanup()\n",
        "    optimizer = torch.optim.Adam(params = [p for p in model.parameters() if p.requires_grad], lr=lr)\n",
        "\n",
        "    ewm_loss = 0\n",
        "    step = 0\n",
        "    model.train()\n",
        "\n",
        "    for epoch in trange(max_epochs):\n",
        "        print(step, max_steps)\n",
        "        if step >= max_steps:\n",
        "            break\n",
        "        tq = tqdm(train_dataloader)\n",
        "        for i, batch in enumerate(tq):\n",
        "            try:\n",
        "                batch['labels'][batch['labels']==0] = -100\n",
        "                out = model(**{k: v.to(model.device) for k, v in batch.items()})\n",
        "                loss = out.loss\n",
        "                loss.backward()\n",
        "            except Exception as e:\n",
        "                print('error on step', i, e)\n",
        "                loss = None\n",
        "                cleanup()\n",
        "                continue\n",
        "            if i and i % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                step += 1\n",
        "                if step >= max_steps:\n",
        "                    break\n",
        "\n",
        "            if i % cleanup_step == 0:\n",
        "                cleanup()\n",
        "\n",
        "            w = 1 / min(i+1, window)\n",
        "            ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
        "            tq.set_description(f'loss: {ewm_loss:4.4f}')\n",
        "\n",
        "            if (i and i % report_step == 0 or i == len(train_dataloader)-1)  and val_dataloader is not None:\n",
        "                model.eval()   \n",
        "                eval_loss = evaluate_model(model, val_dataloader)\n",
        "                model.train()\n",
        "                print(f'epoch {epoch}, step {i}/{step}: train loss: {ewm_loss:4.4f}  val loss: {eval_loss:4.4f}')\n",
        "                \n",
        "            if step % 1000 == 0:\n",
        "                model.save_pretrained(f't5_base_{dname}_{steps}')\n",
        "        \n",
        "    cleanup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrHlRIYsUHos"
      },
      "outputs": [],
      "source": [
        "def train_model(x, y, model_name, test_size=0.1, batch_size=32, **kwargs):\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name).cuda()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    x1, x2, y1, y2 = train_test_split(x, y, test_size=test_size, random_state=42)\n",
        "    train_dataset = PairsDataset(tokenizer(x1), tokenizer(y1))\n",
        "    test_dataset = PairsDataset(tokenizer(x2), tokenizer(y2))\n",
        "    \n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
        "    val_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
        "\n",
        "    train_loop(model, train_dataloader, val_dataloader, **kwargs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep5IQ33rUHot"
      },
      "outputs": [],
      "source": [
        "model_name = 'sberbank-ai/ruT5-base'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zReBInX8UHot"
      },
      "outputs": [],
      "source": [
        "cleanup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tpV9GSuUHot"
      },
      "outputs": [],
      "source": [
        "datasets = {\n",
        "    'train': df\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parametrs = [[2e-5, 1, 100]]\n",
        "steps = 20000\n",
        "for lr, gr, w in parametrs:\n",
        "    for dname, d in datasets.items():\n",
        "        print(f'\\n\\n\\n  {dname}  {steps} \\n=====================\\n\\n')\n",
        "        model = train_model(d['toxic_comment'].tolist(), d['neutral_comment'].tolist(), model_name=model_name, batch_size=8, max_epochs=1000, max_steps=steps, lr=lr, gradient_accumulation_steps=gr, window=w)\n",
        "        model.save_pretrained(f't5_base_{dname}_params_{steps}_{lr}_{gr}_{w}')"
      ],
      "metadata": {
        "id": "23r4wrpIlP09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwb4joYdUHou"
      },
      "source": [
        "### INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSWZj4CFUHou"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/data/input/test.tsv', sep='\\t')\n",
        "toxic_inputs = df['toxic_comment'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zmnBtCHUHou"
      },
      "outputs": [],
      "source": [
        "base_model_name = 'sberbank-ai/ruT5-base'\n",
        "model_name = 't5_base_train_params_20000_2e-05_1_100'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TubpU-RRUHou"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNxCKWuKUHou"
      },
      "outputs": [],
      "source": [
        "model.cuda();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGD5uc18UHou"
      },
      "outputs": [],
      "source": [
        "def paraphrase(text, model, n=None, max_length='auto', temperature=0.0, beams=3):\n",
        "    texts = [text] if isinstance(text, str) else text\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(model.device)\n",
        "    if max_length == 'auto':\n",
        "        max_length = int(inputs.shape[1] * 1.2) + 10\n",
        "    result = model.generate(\n",
        "        inputs, \n",
        "        num_return_sequences=n or 1, \n",
        "        do_sample=False, \n",
        "        temperature=temperature, \n",
        "        repetition_penalty=3.0, \n",
        "        max_length=max_length,\n",
        "        bad_words_ids=[[2]],  # unk\n",
        "        num_beams=beams,\n",
        "    )\n",
        "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
        "    if not n and isinstance(text, str):\n",
        "        return texts[0]\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "2913c55522f643a2ae388dc08f8caa0b"
          ]
        },
        "id": "cG7hDQuyUHou",
        "outputId": "bef325a7-6f02-49a7-9c3a-51ab54d651d1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2913c55522f643a2ae388dc08f8caa0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/110 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "para_results = []\n",
        "problematic_batch = [] \n",
        "batch_size = 8\n",
        "\n",
        "for i in tqdm(range(0, len(toxic_inputs), batch_size)):\n",
        "    batch = [sentence for sentence in toxic_inputs[i:i + batch_size]]\n",
        "    try:\n",
        "        para_results.extend(paraphrase(batch, model, temperature=0.0))\n",
        "    except Exception as e:\n",
        "        print(i)\n",
        "        para_results.append(toxic_inputs[i:i + batch_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAnHj6LwUHov"
      },
      "outputs": [],
      "source": [
        "with open('t5_base.txt', 'w', encoding=\"utf-8\") as file:\n",
        "    file.writelines([sentence+'\\n' for sentence in para_results])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}